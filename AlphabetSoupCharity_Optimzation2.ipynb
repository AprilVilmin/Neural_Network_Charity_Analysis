{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphabetSoupCharity_Optimzation2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5tn+cK2L4W9WlfadqJR8K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AprilVilmin/Neural_Network_Charity_Analysis/blob/main/AlphabetSoupCharity_Optimzation2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4WwOl9qrk3o"
      },
      "outputs": [],
      "source": [
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "#  Import and read the charity_data.csv.\n",
        "import pandas as pd \n",
        "application_df = pd.read_csv(\"charity_data.csv\")\n",
        "application_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
        "# https://stackoverflow.com/questions/40389018/dropping-multiple-columns-from-a-data-frame-using-python\n",
        "application_df.drop(['EIN', 'NAME', 'SPECIAL_CONSIDERATIONS', 'AFFILIATION'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "3pvtN0udroPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of unique values in each column.\n",
        "application_df.nunique()"
      ],
      "metadata": {
        "id": "d7ppLaVsrrNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at APPLICATION_TYPE value counts for binning\n",
        "application_counts = application_df.APPLICATION_TYPE.value_counts()\n",
        "application_counts"
      ],
      "metadata": {
        "id": "Ey5JYVqort2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the value counts of APPLICATION_TYPE\n",
        "application_counts.plot.density()"
      ],
      "metadata": {
        "id": "PU4gnqKirxYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which values to replace if counts are less than ...?\n",
        "replace_application = list(application_counts[application_counts < 500].index)\n",
        "\n",
        "# Replace in dataframe\n",
        "for app in replace_application:\n",
        "    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,\"Other\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "application_df.APPLICATION_TYPE.value_counts()"
      ],
      "metadata": {
        "id": "vOJwNyY2r0x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at CLASSIFICATION value counts for binning\n",
        "class_counts = application_df.CLASSIFICATION.value_counts()\n",
        "class_counts"
      ],
      "metadata": {
        "id": "pH35fqEor37v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the value counts of CLASSIFICATION\n",
        "class_counts.plot.density()"
      ],
      "metadata": {
        "id": "YvAz6Sg7r7kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine which values to replace if counts are less than ..?\n",
        "replace_class= list(class_counts[class_counts < 500].index)\n",
        "\n",
        "# Replace in dataframe\n",
        "for cls in replace_class:\n",
        "    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,\"Other\")\n",
        "    \n",
        "# Check to make sure binning was successful\n",
        "application_df.CLASSIFICATION.value_counts()"
      ],
      "metadata": {
        "id": "12f1BczVr-G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate our categorical variable lists\n",
        "application_cat = application_df.dtypes[application_df.dtypes == \"object\"].index.tolist()"
      ],
      "metadata": {
        "id": "PufsWOWdsEBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a OneHotEncoder instance\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit and transform the OneHotEncoder using the categorical variable list\n",
        "encode_df = pd.DataFrame(enc.fit_transform(application_df[application_cat]))\n",
        "\n",
        "# Add the encoded variable names to the dataframe\n",
        "encode_df.columns = enc.get_feature_names(application_cat)\n",
        "encode_df.head()"
      ],
      "metadata": {
        "id": "ymSqtjAbsG3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "application_df = application_df.merge(encode_df,left_index=True, right_index=True)\n",
        "application_df = application_df.drop(application_cat,1)\n",
        "application_df.head()"
      ],
      "metadata": {
        "id": "p2cwV_tAsJfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = application_df[\"IS_SUCCESSFUL\"].values\n",
        "X = application_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
        "\n",
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
      ],
      "metadata": {
        "id": "nXq6qVZDsOGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler instances\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "qPWNP4w9sSrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
        "number_input_features = len(X_train_scaled[0])\n",
        "hidden_nodes_layer1 = 300\n",
        "hidden_nodes_layer2 = 250\n",
        "hidden_nodes_layer3 = 200\n",
        "hidden_nodes_layer4 = 150\n",
        "hidden_nodes_layer5 = 100\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Third hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
        "\n",
        "# Fourth hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
        "\n",
        "# Fifth hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn.summary()"
      ],
      "metadata": {
        "id": "czITH1xZsWi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "j-68wubtsa2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "metadata": {
        "id": "QvSi94sBsexp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = keras.callbacks.ModelCheckpoint('model{epoch:08d}.h5', period=5) \n",
        "nn.fit(X_train_scaled, y_train, callbacks=[checkpoint])"
      ],
      "metadata": {
        "id": "ren5ndA7shVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8n20tyEslG-",
        "outputId": "d87f30c5-6506-4488-896a-43a3ab0c119b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "804/804 [==============================] - 28s 35ms/step - loss: 0.6096 - accuracy: 0.6541\n",
            "Epoch 2/50\n",
            "804/804 [==============================] - 26s 32ms/step - loss: 0.6095 - accuracy: 0.6530\n",
            "Epoch 3/50\n",
            "804/804 [==============================] - 22s 28ms/step - loss: 0.6076 - accuracy: 0.6563\n",
            "Epoch 4/50\n",
            "804/804 [==============================] - 22s 27ms/step - loss: 0.6081 - accuracy: 0.6562\n",
            "Epoch 5/50\n",
            "804/804 [==============================] - 21s 26ms/step - loss: 0.6077 - accuracy: 0.6546\n",
            "Epoch 6/50\n",
            "804/804 [==============================] - 23s 28ms/step - loss: 0.6075 - accuracy: 0.6569\n",
            "Epoch 7/50\n",
            " 31/804 [>.............................] - ETA: 21s - loss: 0.5845 - accuracy: 0.6815"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "uACeUDHvspSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10: Save and export your results to an HDF5 file, and name it AlphabetSoupCharity.h5\n",
        "# Copied code from https://github.com/PopeScooby/Neural_Network_Charity_Analysis/blob/main/AlphabetSoupCharity.ipynb\n",
        "nn.save(\"AlphabetSoupCharity_Optimzation2.h5\")"
      ],
      "metadata": {
        "id": "8tzS0JzzstSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}